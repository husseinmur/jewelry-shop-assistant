import streamlit as st
from PIL import Image
import json
from datetime import datetime
from shared.config import init_apis, TEXT_MODEL
# from shared.langchain_rag import init_langchain_rag  # No longer needed
from shared.embeddings import get_image_description
# from shared.database import search_by_image  # No longer needed - using optimized search
import openai

# Page config
st.set_page_config(
    page_title="Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ¬Ø± Ø§Ù„Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª",
    page_icon="ğŸ’",
    layout="wide"
)

# Add RTL CSS and chat input positioning
st.markdown("""
<style>
    .main .block-container {
        direction: rtl;
        text-align: right;
        padding-bottom: 100px;
    }
    .stChatMessage {
        direction: rtl;
        text-align: right;
    }
    .stTextInput > div > div > input {
        direction: rtl;
        text-align: right;
    }
    .stFileUploader > div {
        direction: rtl;
        text-align: right;
    }
    h1 {
        direction: rtl;
        text-align: right;
    }
    .stRadio {
        direction: rtl !important;
        text-align: right !important;
    }
    .stRadio > div[role="radiogroup"] {
        direction: rtl !important;
        justify-content: flex-end !important;
        display: flex !important;
        flex-direction: row-reverse !important;
    }
    .stRadio > div[role="radiogroup"] > label {
        direction: rtl !important;
        margin-left: 1rem !important;
        margin-right: 0 !important;
    }
    .stRadio div[data-testid="stHorizontalBlock"] {
        direction: rtl !important;
        justify-content: flex-end !important;
    }
    .stChatInputContainer {
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        background: white;
        z-index: 1000;
        padding: 10px 20px;
        border-top: 1px solid #e0e0e0;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state for chat history
if "messages" not in st.session_state:
    st.session_state.messages = []
    # Add initial assistant message
    st.session_state.messages.append({
        "role": "assistant",
        "content": "Ù…Ø±Ø­Ø¨Ø§Ù‹! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ"
    })

# RAG system no longer needed - using direct Pinecone + LLM verification

if "active_tab" not in st.session_state:
    st.session_state.active_tab = "chat"



# Initialize APIs only (no RAG system needed)
try:
    openai_client, pinecone_index = init_apis()

    if not openai_client or not pinecone_index:
        st.error("âŒ ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø­Ø«")
        st.stop()

except Exception as e:
    st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„: {e}")
    st.stop()

st.title("ğŸ’ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ¬Ø± Ø§Ù„Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª")

def llm_filter_results(query: str, results: list, openai_client) -> list:
    """Use LLM to intelligently filter search results for relevance"""
    try:
        if not results:
            return []

        # Format results for verification
        results_text = ""
        for i, result in enumerate(results):
            metadata = result.metadata
            results_text += f"ID: {result.id}\n"
            results_text += f"Name: {metadata.get('name', 'N/A')}\n"
            results_text += f"Category: {metadata.get('category', 'N/A')}\n"
            results_text += f"Description: {metadata.get('description', 'N/A')[:100]}...\n"
            results_text += f"Score: {result.score:.3f}\n\n"

        verification_prompt = f"""
Query: "{query}"

Available products to evaluate:
{results_text}

Task: Return product IDs that would satisfy the customer's search intent.

IMPORTANT CONTEXT & FLEXIBILITY RULES:
ğŸ“¿ JEWELRY USE CASES - Understand customer intent:
- "Ø®Ø§ØªÙ… Ø®Ø·ÙˆØ¨Ø©" (engagement ring) = elegant rings with stones, gold, suitable for proposals
- "Ø®Ø§ØªÙ… Ø²ÙˆØ§Ø¬" (wedding ring) = classic rings, bands, gold/platinum, suitable for marriage
- "Ø®Ø§ØªÙ… Ø¨Ø³ÙŠØ·" (simple ring) = minimal design, clean lines, not overly decorative
- "Ø¹Ù‚Ø¯ ÙØ§Ø®Ø±" (luxury necklace) = high-end necklaces, precious materials, sophisticated design
- "Ø£Ù‚Ø±Ø§Ø· ÙŠÙˆÙ…ÙŠØ©" (daily earrings) = comfortable, suitable for everyday wear
- "Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª Ù‡Ø¯ÙŠØ©" (gift jewelry) = presentable pieces, nice packaging appeal

ğŸ¯ MATCHING STRATEGY:
- Focus on SUITABILITY for the intended use, not exact terminology
- A beautiful gold ring with stones IS suitable for engagement even if not labeled "engagement ring"
- A simple gold band IS suitable for wedding even if not labeled "wedding ring"
- Consider material, design style, and appropriateness for the occasion

ğŸ’ MATERIAL & STYLE UNDERSTANDING:
- "Ø°Ù‡Ø¨" includes all gold types (yellow, white, rose gold)
- "Ø¨Ø³ÙŠØ·" means clean, minimal, not overly decorative
- "ÙØ§Ø®Ø±" means luxury materials, sophisticated design, higher quality
- "Ø£Ù†ÙŠÙ‚" means elegant, refined, sophisticated

âœ… EXAMPLES:
Query: "Ø®Ø§ØªÙ… Ø®Ø·ÙˆØ¨Ø©" â†’ Return: elegant rings with stones, gold rings suitable for proposals
Query: "Ø³Ù„Ø³Ù„Ø© Ø¨Ø³ÙŠØ·Ø©" â†’ Return: minimal necklaces, clean design chains
Query: "Ø£Ù‚Ø±Ø§Ø· Ø°Ù‡Ø¨" â†’ Return: any gold earrings regardless of specific style

âŒ ONLY EXCLUDE if products are completely wrong category or material
- Query: "Ø®Ø§ØªÙ…" (ring) â†’ Don't return necklaces or earrings
- Query: "Ø°Ù‡Ø¨" (gold) â†’ Don't return silver-only items

Return ONLY a JSON list of product IDs that match the customer's intent: ["id1", "id2", "id3"] or []
"""

        response = openai_client.chat.completions.create(
            model="gpt-5-nano-2025-08-07",
            messages=[{"role": "user", "content": verification_prompt}],
            temperature=1.0,
            max_completion_tokens=2000
        )

        # Parse response to get filtered IDs
        response_text = response.choices[0].message.content.strip()
        print(f"ğŸ› DEBUG LLM Response: '{response_text}' (length: {len(response_text)})")
        try:
            import json
            filtered_ids = json.loads(response_text)
            if not isinstance(filtered_ids, list):
                print(f"ğŸ› DEBUG - Response not a list: {type(filtered_ids)}")
                filtered_ids = []
            else:
                print(f"ğŸ› DEBUG - Parsed {len(filtered_ids)} IDs successfully")
        except Exception as e:
            print(f"ğŸ› DEBUG - JSON parsing failed: {e}")
            filtered_ids = []

        # Filter original results by verified IDs
        filtered_results = [r for r in results if r.id in filtered_ids]
        print(f"ğŸ› DEBUG LLM Filter - Input: {len(results)}, LLM IDs: {len(filtered_ids)}, Output: {len(filtered_results)}")
        return filtered_results

    except Exception as e:
        print(f"ğŸ› DEBUG LLM Filter - Exception: {e}")
        st.error(f"Error in LLM filtering: {e}")
        # Fallback to similarity filtering
        fallback_results = [r for r in results if r.score >= 0.4][:5]
        print(f"ğŸ› DEBUG LLM Filter - Using fallback: {len(fallback_results)} results")
        return fallback_results

def search_jewelry_products(query: str, conversation_history: list = None) -> str:
    """Search for jewelry products using direct Pinecone + LLM verification"""
    try:
        if not pinecone_index:
            return "Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø­Ø« ØºÙŠØ± Ù…ØªØ§Ø­ Ø­Ø§Ù„ÙŠØ§Ù‹."

        # Get text embedding for the query
        from shared.embeddings import get_text_embedding
        query_embedding = get_text_embedding(query)

        if not query_embedding:
            return "ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…."

        # Search Pinecone directly
        pinecone_results = pinecone_index.query(
            vector=query_embedding,
            top_k=8,  # Reduced from 15 to avoid overwhelming LLM
            include_metadata=True
        )

        # Basic similarity filter first (fast)
        decent_results = [r for r in pinecone_results.matches if r.score >= 0.3]

        if not decent_results:
            return "NO_RESULTS_NEED_CLARIFICATION"

        # LLM verification filter (intelligent)
        filtered_results = llm_filter_results(query, decent_results, openai_client)

        if not filtered_results:
            return "NO_RESULTS_NEED_CLARIFICATION"

        # Format results for LLM context
        final_results = filtered_results[:5]  # Top 5 verified results
        products_info = f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(final_results)} Ù…Ù†ØªØ¬ Ù…Ø·Ø§Ø¨Ù‚ ÙÙŠ Ø§Ù„Ù…Ø®Ø²ÙˆÙ†:\n\n"

        for i, result in enumerate(final_results, 1):
            metadata = result.metadata
            products_info += f"{i}. {metadata.get('name', 'Ù…Ù†ØªØ¬')}\n"
            products_info += f"   Ø§Ù„Ø³Ø¹Ø±: {metadata.get('price', 0):.2f} Ø±ÙŠØ§Ù„\n"
            products_info += f"   Ø§Ù„ÙØ¦Ø©: {metadata.get('category', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}\n"
            if metadata.get('karat'):
                products_info += f"   Ø§Ù„Ø¹ÙŠØ§Ø±: {metadata.get('karat')}\n"
            if metadata.get('weight', 0) > 0:
                products_info += f"   Ø§Ù„ÙˆØ²Ù†: {metadata.get('weight')} Ø¬Ø±Ø§Ù…\n"
            if metadata.get('design'):
                products_info += f"   Ø§Ù„ØªØµÙ…ÙŠÙ…: {metadata.get('design')}\n"
            if metadata.get('product_url'):
                products_info += f"   Ø§Ù„Ø±Ø§Ø¨Ø·: {metadata.get('product_url')}\n"
            products_info += f"   Ø§Ù„ÙˆØµÙ: {metadata.get('description', '')[:150]}...\n"
            products_info += f"   Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ·Ø§Ø¨Ù‚: {result.score * 100:.1f}%\n\n"

        # Add instruction for LLM
        products_info += "\nØªØ¹Ù„ÙŠÙ…Ø§Øª: ØªØ­Ø¯Ø« Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø¯Ø§ÙØ¦ ÙˆÙ…Ø±Ø­Ø¨ ÙˆÙˆØ¯ÙˆØ¯. Ø§Ø°ÙƒØ± Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒ Ù…Ø¹ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©. ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹. ØªØ°ÙƒØ±: Ø£Ù†Øª ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØªØ±Ø¨Ø· Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ù…Ø§ ØªÙ… Ù…Ù†Ø§Ù‚Ø´ØªÙ‡ Ø³Ø§Ø¨Ù‚Ø§Ù‹."

        return products_info

    except Exception as e:
        return f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}"

def ask_clarifying_questions(reason: str, questions: list) -> str:
    """Handle clarifying questions for vague queries"""
    try:
        response = f"Ø£Ø±ÙŠØ¯ Ø£Ù† Ø£Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø§Ù„Ù‚Ø·Ø¹! ğŸ˜Š\n\n"
        response += f"{reason}\n\n"
        response += "Ù„Ø°Ù„ÙƒØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø³Ø§Ø¹Ø¯ØªÙŠ Ø¨Ø¨Ø¹Ø¶ Ø§Ù„ØªÙØ§ØµÙŠÙ„:\n\n"

        for i, question in enumerate(questions, 1):
            response += f"{i}. {question}\n"

        response += f"\nØ¨Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø³Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø¹Ø±Ø¶ Ø£ÙØ¶Ù„ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„ØªÙŠ ØªÙ†Ø§Ø³Ø¨ Ø°ÙˆÙ‚Ùƒ ØªÙ…Ø§Ù…Ø§Ù‹! âœ¨"

        return response

    except Exception as e:
        return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {e}"

def get_ai_response_for_image_search(image_description: str, conversation_history: list) -> str:
    """Special function for image search that doesn't ask clarifying questions"""
    try:
        # Get OpenAI client
        openai_client, pinecone_index = init_apis()

        # Search for products based on image description
        search_result = search_jewelry_products(image_description, conversation_history)

        # If no results found, return appropriate message for image search
        if search_result == "NO_RESULTS_NEED_CLARIFICATION":
            return "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø·Ø¹ Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù„ØµÙˆØ±Ø© Ø§Ù„ØªÙŠ Ø±ÙØ¹ØªÙ‡Ø§ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹ØªÙ†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠØ©. ğŸ˜”\n\nÙŠÙ…ÙƒÙ†Ùƒ ØªØ¬Ø±Ø¨Ø©:\nâ€¢ Ø±ÙØ¹ ØµÙˆØ±Ø© Ø£Ø®Ø±Ù‰ Ø£Ùˆ Ø¨Ø²Ø§ÙˆÙŠØ© Ù…Ø®ØªÙ„ÙØ©\nâ€¢ ÙˆØµÙ Ø§Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„ØªÙŠ ØªØ¨Ø­Ø« Ø¹Ù†Ù‡Ø§ Ù†ØµÙŠØ§Ù‹\nâ€¢ ØªØµÙØ­ Ù…Ø¬Ù…ÙˆØ¹ØªÙ†Ø§ Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø·Ø¹ Ù…Ø´Ø§Ø¨Ù‡Ø© ğŸ’"

        # If results found, create response
        image_query = f"ÙˆØµÙ Ø§Ù„ØµÙˆØ±Ø©: {image_description}\n\nÙ†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«:\n{search_result}"

        # Create simple response for image analysis
        response = openai_client.chat.completions.create(
            model="gpt-5-nano-2025-08-07",
            messages=[
                {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨ÙŠØ¹Ø§Øª ÙˆØ¯ÙˆØ¯ ÙÙŠ Ù…ØªØ¬Ø± Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª. Ø­Ù„Ù„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø© ÙˆØ§Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø© Ø¨Ø­Ù…Ø§Ø³. Ø§Ø°ÙƒØ± Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙÙŠ Ø§Ù„ØªØµÙ…ÙŠÙ… Ø£Ùˆ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø£Ùˆ Ø§Ù„Ø·Ø±Ø§Ø². ÙƒÙ† ÙˆØ¯ÙˆØ¯ ÙˆÙ…ØªØ­Ù…Ø³."},
                {"role": "user", "content": image_query}
            ],
            temperature=1.0
        )

        return response.choices[0].message.content

    except Exception as e:
        return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {e}"

def get_ai_response_with_tools(user_message: str, conversation_history: list) -> str:
    """Get AI response with access to search tools and full conversation context"""
    try:
        # Define the search tool
        search_tool = {
            "type": "function",
            "function": {
                "name": "search_jewelry_products",
                "description": "Search for jewelry products in the store inventory when customer asks about specific products or wants to see what's available",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "Search query for jewelry products"
                        }
                    },
                    "required": ["query"]
                }
            }
        }

        # Define the clarification tool
        ask_clarification_tool = {
            "type": "function",
            "function": {
                "name": "ask_clarifying_questions",
                "description": "Ask clarifying questions when the customer's request is too vague or lacks important details for a good search",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "reason": {
                            "type": "string",
                            "description": "Why clarification is needed"
                        },
                        "questions": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "List of clarifying questions to ask"
                        }
                    },
                    "required": ["reason", "questions"]
                }
            }
        }

        # Build conversation context summary FIRST
        context_summary = ""
        if conversation_history:
            recent_history = conversation_history[-4:] if len(conversation_history) > 4 else conversation_history
            if recent_history:
                context_summary = "ğŸ“‹ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n"
                for msg in recent_history:
                    if msg["role"] == "user":
                        context_summary += f"Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù‚Ø§Ù„: {msg['content']}\n"
                    elif msg["role"] == "assistant":
                        context_summary += f"Ø£Ù†Øª Ø£Ø¬Ø¨Øª: {msg['content'][:100]}...\n"

                # Extract key information
                mentioned_products = []
                for msg in recent_history:
                    content = msg['content'].lower()
                    if any(product in content for product in ['Ø®Ø§ØªÙ…', 'Ø¹Ù‚Ø¯', 'Ø³Ù„Ø³Ù„Ø©', 'Ø£Ù‚Ø±Ø§Ø·', 'Ø³ÙˆØ§Ø±']):
                        mentioned_products.append(msg['content'][:150])

                if mentioned_products:
                    context_summary += f"\nğŸ¯ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ø°ÙƒØ±Ù‡Ø§:\n"
                    for product in mentioned_products[-2:]:  # Last 2 product mentions
                        context_summary += f"- {product}\n"

        # Prepare messages with CONTEXT FIRST
        messages = [
            {
                "role": "system",
                "content": f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨ÙŠØ¹Ø§Øª Ø°ÙƒÙŠ ÙˆÙˆØ¯ÙˆØ¯ ÙÙŠ Ù…ØªØ¬Ø± Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª.

{context_summary}

ğŸ”— CRITICAL: Ø¥Ø°Ø§ Ø³Ø£Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¹Ù† "Ø§Ù„Ø³Ø¹Ø±" Ø£Ùˆ "Ø§Ù„Ø£Ù„ÙˆØ§Ù†" Ø£Ùˆ "Ù…ØªÙˆÙØ±" Ø¨Ø¯ÙˆÙ† ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù†ØªØ¬ØŒ
ÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ø¨Ø· Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¢Ø®Ø± Ù…Ù†ØªØ¬ Ø°ÙƒØ±ØªÙ‡ ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø£Ø¹Ù„Ø§Ù‡.

Ù‚ÙˆØ§Ø¹Ø¯ Ø£Ø³Ø§Ø³ÙŠØ©:
1. Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø£ÙˆÙ„Ø§Ù‹ Ù‚Ø¨Ù„ Ø£ÙŠ Ø´ÙŠØ¡
2. Ø§Ø±Ø¨Ø· Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØºØ§Ù…Ø¶Ø© Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚
3. Ù„Ø¯ÙŠÙƒ Ø£Ø¯Ø§ØªØ§Ù† Ù…Ù‡Ù…ØªØ§Ù†:
   - search_jewelry_products: Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù†ØªØ¬Ø§Øª Ù…Ø­Ø¯Ø¯Ø©
   - ask_clarifying_questions: Ù„Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© ØªÙˆØ¶ÙŠØ­ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
4. Ù„Ø§ ØªØ®ØªØ±Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª - Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ù…Ø§ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø£Ùˆ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
5. ÙƒÙ† ÙˆØ¯ÙˆØ¯Ø§Ù‹ ÙˆÙ…ØªØ­Ù…Ø³Ø§Ù‹

ğŸ¤” Ù…ØªÙ‰ ØªØ·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© ØªÙˆØ¶ÙŠØ­ÙŠØ©:

ğŸš¨ ASK CLARIFICATION for these vague cases:
- "jewelry" or "Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª" ALONE (no type at all)
- "gift" or "Ù‡Ø¯ÙŠØ©" ALONE (no details at all)
- "something nice" or "Ø´ÙŠØ¡ Ø¬Ù…ÙŠÙ„" (completely vague)
- "ring" or "Ø®Ø§ØªÙ…" ALONE (type only, needs material/style/occasion)
- "necklace" or "Ø¹Ù‚Ø¯" ALONE (type only, needs material/style)
- "earrings" or "Ø£Ù‚Ø±Ø§Ø·" ALONE (type only, needs material/style)

âœ… SEARCH DIRECTLY - These have enough specificity:
âœ… "gold ring" â†’ SEARCH (type + material)
âœ… "Ø®Ø§ØªÙ… Ø°Ù‡Ø¨" â†’ SEARCH (type + material)
âœ… "silver earrings" â†’ SEARCH (type + material)
âœ… "Ø£Ù‚Ø±Ø§Ø· ÙØ¶Ø©" â†’ SEARCH (type + material)
âœ… "simple necklace" â†’ SEARCH (type + style)
âœ… "Ø¹Ù‚Ø¯ Ø¨Ø³ÙŠØ·" â†’ SEARCH (type + style)
âœ… "wedding ring" â†’ SEARCH (type + occasion)
âœ… "Ø®Ø§ØªÙ… Ø²ÙˆØ§Ø¬" â†’ SEARCH (type + occasion)
âœ… "engagement ring" â†’ SEARCH (type + occasion)
âœ… "Ø®Ø§ØªÙ… Ø®Ø·ÙˆØ¨Ø©" â†’ SEARCH (type + occasion)

â“ ASK CLARIFICATION:
â“ "ring" â†’ ASK (needs material/style/occasion)
â“ "Ø®Ø§ØªÙ…" â†’ ASK (needs material/style/occasion)
â“ "necklace" â†’ ASK (needs material/style)
â“ "Ø¹Ù‚Ø¯" â†’ ASK (needs material/style)
â“ "jewelry" â†’ ASK (no type specified)
â“ "Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª" â†’ ASK (no type specified)
â“ "gift" â†’ ASK (no details at all)
â“ "Ù‡Ø¯ÙŠØ©" â†’ ASK (no details at all)

ğŸ¯ Key Rule: Customer needs TYPE + at least ONE additional detail (material/style/occasion) to search!

âš ï¸ CRITICAL: If customer says ONLY "ring", "Ø®Ø§ØªÙ…", "necklace", "Ø¹Ù‚Ø¯", "earrings", or "Ø£Ù‚Ø±Ø§Ø·" without any additional details, you MUST use ask_clarifying_questions tool!

ğŸ¯ Ù…Ø«Ø§Ù„: Ø¥Ø°Ø§ Ø°ÙƒØ±Øª Ø®ÙˆØ§ØªÙ… Ø³Ø§Ø¨Ù‚Ø§Ù‹ ÙˆØ³Ø£Ù„ "ÙƒÙ… Ø§Ù„Ø³Ø¹Ø±ØŸ" â†’ Ø£Ø¬Ø¨ Ø¹Ù† Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø®ÙˆØ§ØªÙ… Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©."""
            }
        ]

        # Add recent conversation history
        recent_history = conversation_history[-3:] if len(conversation_history) > 3 else conversation_history
        for msg in recent_history:
            if msg["role"] in ["user", "assistant"]:
                messages.append({"role": msg["role"], "content": msg["content"]})

        # Add current user message
        messages.append({"role": "user", "content": user_message})

        # Call OpenAI with function calling
        response = openai.chat.completions.create(
            model="gpt-5-nano-2025-08-07",
            messages=messages,
            tools=[search_tool, ask_clarification_tool],
            tool_choice="auto",  # Let AI decide when to use tools
            temperature=1.0
        )

        response_message = response.choices[0].message

        # Check if AI wants to use tools
        if response_message.tool_calls:
            for tool_call in response_message.tool_calls:
                function_args = json.loads(tool_call.function.arguments)

                if tool_call.function.name == "search_jewelry_products":
                    # Extract search query
                    search_query = function_args.get("query", "")

                    # Perform search
                    search_result = search_jewelry_products(search_query, conversation_history)

                    # Check if search failed and needs clarification
                    if search_result == "NO_RESULTS_NEED_CLARIFICATION":
                        # Automatically trigger clarification instead of showing failure
                        default_questions = [
                            "Ù…Ø§ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª Ø§Ù„ØªÙŠ ØªØ¨Ø­Ø« Ø¹Ù†Ù‡Ø§ØŸ (Ø®Ø§ØªÙ…ØŒ Ø¹Ù‚Ø¯ØŒ Ø£Ù‚Ø±Ø§Ø·ØŒ Ø³ÙˆØ§Ø±)",
                            "Ù…Ø§ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©ØŸ (Ø²ÙˆØ§Ø¬ØŒ Ø®Ø·ÙˆØ¨Ø©ØŒ Ù‡Ø¯ÙŠØ©ØŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÙŠÙˆÙ…ÙŠ)",
                            "Ù…Ø§ Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù…ÙØ¶Ù„Ø©ØŸ (Ø°Ù‡Ø¨ØŒ ÙØ¶Ø©ØŒ Ø£Ø­Ø¬Ø§Ø± ÙƒØ±ÙŠÙ…Ø©)",
                            "Ù…Ø§ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…ÙØ¶Ù„ØŸ (Ø¨Ø³ÙŠØ·ØŒ ÙØ§Ø®Ø±ØŒ Ø¹ØµØ±ÙŠØŒ ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ)"
                        ]
                        return ask_clarifying_questions(
                            "Ø£Ø±ÙŠØ¯ Ø£Ù† Ø£Ø³Ø§Ø¹Ø¯Ùƒ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ©! ğŸ’",
                            default_questions
                        )

                    # Add tool result to conversation
                    messages.append(response_message)
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": search_result
                    })

                    # Get final response with search results
                    final_response = openai.chat.completions.create(
                        model="gpt-5-nano-2025-08-07",
                        messages=messages,
                        temperature=1.0
                    )

                    return final_response.choices[0].message.content

                elif tool_call.function.name == "ask_clarifying_questions":
                    # Extract clarification parameters
                    reason = function_args.get("reason", "")
                    questions = function_args.get("questions", [])

                    # Generate clarification response
                    clarification_result = ask_clarifying_questions(reason, questions)

                    return clarification_result

        # No tool call needed, return direct response
        return response_message.content

    except Exception as e:
        return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {e}"

def display_products(products):
    """Display product results in a nice format"""
    if not products:
        return

    cols = st.columns(min(len(products), 3))
    for idx, result in enumerate(products):
        with cols[idx % 3]:
            metadata = result['metadata']

            with st.container():
                st.markdown(f"**{metadata.get('name', 'Ù…Ù†ØªØ¬')}**")
                st.markdown(f"ğŸ’° **{metadata.get('price', 0):.2f} Ø±ÙŠØ§Ù„**")
                st.markdown(f"ğŸ“‚ {metadata.get('category', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")

                # Show additional details
                details = []
                if metadata.get('karat'):
                    details.append(f"Ø§Ù„Ø¹ÙŠØ§Ø±: {metadata.get('karat')}")
                if metadata.get('weight', 0) > 0:
                    details.append(f"Ø§Ù„ÙˆØ²Ù†: {metadata.get('weight')} Ø¬Ø±Ø§Ù…")
                if metadata.get('design'):
                    details.append(f"Ø§Ù„ØªØµÙ…ÙŠÙ…: {metadata.get('design')}")

                if details:
                    st.markdown(f"ğŸ”¹ {' | '.join(details)}")

                # Description (truncated)
                description = metadata.get('description', '')
                if len(description) > 100:
                    description = description[:100] + "..."
                st.markdown(f"ğŸ“ {description}")

                # Match score
                if 'score' in result:
                    st.markdown(f"ğŸ¯ ØªØ·Ø§Ø¨Ù‚: {result['score'] * 100:.1f}%")

                # Add to cart button
                if st.button(f"ğŸ›’ Ø£Ø¶Ù Ù„Ù„Ø³Ù„Ø©", key=f"cart_{result['id']}_{idx}"):
                    st.success("ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù†ØªØ¬ Ù„Ù„Ø³Ù„Ø©! ğŸ›ï¸")

                st.markdown("---")

# Tab selection with radio buttons - using columns to align right
col1, col2 = st.columns([3, 1])

with col2:
    tab_choice = st.radio(
        "Ø§Ø®ØªØ± Ø§Ù„Ù†Ø´Ø§Ø·:",
        ["ğŸ“¸ Ø¨Ø­Ø« Ø¨Ø§Ù„ØµÙˆØ±Ø©", "ğŸ’¬ Ù…Ø­Ø§Ø¯Ø«Ø© Ù†ØµÙŠØ©"],
        horizontal=True,
        index=1  # Default to "Ù…Ø­Ø§Ø¯Ø«Ø© Ù†ØµÙŠØ©" (text search)
    )

st.session_state.active_tab = "chat" if "Ù…Ø­Ø§Ø¯Ø«Ø©" in tab_choice else "image"

if st.session_state.active_tab == "chat":
    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

elif st.session_state.active_tab == "image":
    # Image upload section
    uploaded_image = st.file_uploader(
        "Ø§Ø®ØªØ± ØµÙˆØ±Ø© Ù‚Ø·Ø¹Ø© Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª:",
        type=['png', 'jpg', 'jpeg'],
        help="ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ ØµÙˆØ±Ø© Ù‚Ø·Ø¹Ø© Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª Ù„Ø£Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù†Ù‡Ø§ Ø£Ùˆ Ø£Ø¬Ø¯ Ù‚Ø·Ø¹ Ù…Ø´Ø§Ø¨Ù‡Ø©"
    )

    if uploaded_image:
        image = Image.open(uploaded_image)
        st.image(image, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©", width=300)

        if st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ø¨Ø­Ø« Ø¹Ù† Ù…Ø´Ø§Ø¨Ù‡Ø©", type="primary"):
            with st.spinner("ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ù„Ø¨Ø­Ø«..."):
                # Analyze the image
                description = get_image_description(image)
                st.info(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©: {description[:100]}...")

                # Use specialized image search function
                bot_response = get_ai_response_for_image_search(description, st.session_state.messages)

                # Display results
                st.markdown(bot_response)

                # Add to chat history
                st.session_state.messages.append({
                    "role": "user",
                    "content": "ğŸ–¼ï¸ Ø±ÙØ¹Øª ØµÙˆØ±Ø© Ù‚Ø·Ø¹Ø© Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª"
                })
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": bot_response
                })
    else:
        st.info("Ø§Ø®ØªØ± ØµÙˆØ±Ø©")

# Chat input - only show on chat tab
if st.session_state.active_tab == "chat":
    if prompt := st.chat_input("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§..."):
        # Display user message immediately
        with st.chat_message("user"):
            st.markdown(prompt)

        # Display assistant thinking and response
        with st.chat_message("assistant"):
            thinking_placeholder = st.empty()
            thinking_placeholder.markdown("ğŸ¤” Ø£ÙÙƒØ±...")

            # Use function calling approach - ONE LLM call with full context and tools
            response = get_ai_response_with_tools(prompt, st.session_state.messages)

            thinking_placeholder.markdown(response)

        # Add to history after display
        st.session_state.messages.extend([
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": response}
        ])

