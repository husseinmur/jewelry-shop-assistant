import streamlit as st
from PIL import Image
import json
from datetime import datetime
from shared.config import init_apis, TEXT_MODEL
from shared.langchain_rag import init_langchain_rag
from shared.embeddings import get_image_description
from shared.database import search_by_image  # Keep for image search
import openai

# Page config
st.set_page_config(
    page_title="Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ¬Ø± Ø§Ù„Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª",
    page_icon="ğŸ’",
    layout="wide"
)

# Add RTL CSS and chat input positioning
st.markdown("""
<style>
    .main .block-container {
        direction: rtl;
        text-align: right;
        padding-bottom: 100px;
    }
    .stChatMessage {
        direction: rtl;
        text-align: right;
    }
    .stTextInput > div > div > input {
        direction: rtl;
        text-align: right;
    }
    .stFileUploader > div {
        direction: rtl;
        text-align: right;
    }
    h1 {
        direction: rtl;
        text-align: right;
    }
    .stRadio {
        direction: rtl !important;
        text-align: right !important;
    }
    .stRadio > div[role="radiogroup"] {
        direction: rtl !important;
        justify-content: flex-end !important;
        display: flex !important;
        flex-direction: row-reverse !important;
    }
    .stRadio > div[role="radiogroup"] > label {
        direction: rtl !important;
        margin-left: 1rem !important;
        margin-right: 0 !important;
    }
    .stRadio div[data-testid="stHorizontalBlock"] {
        direction: rtl !important;
        justify-content: flex-end !important;
    }
    .stChatInputContainer {
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        background: white;
        z-index: 1000;
        padding: 10px 20px;
        border-top: 1px solid #e0e0e0;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state for chat history
if "messages" not in st.session_state:
    st.session_state.messages = []
    # Add initial assistant message
    st.session_state.messages.append({
        "role": "assistant",
        "content": "Ù…Ø±Ø­Ø¨Ø§Ù‹! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ"
    })

if "rag_system" not in st.session_state:
    st.session_state.rag_system = None

if "active_tab" not in st.session_state:
    st.session_state.active_tab = "chat"



# Initialize APIs and RAG
try:
    openai_client, pinecone_index = init_apis()

    # Initialize LangChain RAG system
    if st.session_state.rag_system is None:
        with st.spinner("ØªØ­Ø¶ÙŠØ± Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ..."):
            st.session_state.rag_system = init_langchain_rag(
                pinecone_index,
                st.secrets["OPENAI_API_KEY"]
            )

        if not st.session_state.rag_system:
            st.error("âŒ ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø­Ø«")
            st.stop()

except Exception as e:
    st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„: {e}")
    st.stop()

st.title("ğŸ’ Ù…Ø³Ø§Ø¹Ø¯ Ù…ØªØ¬Ø± Ø§Ù„Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª")

def search_jewelry_products(query: str, conversation_history: list = None) -> str:
    """Search for jewelry products and return formatted results"""
    try:
        if st.session_state.rag_system:
            _, results = st.session_state.rag_system.conversational_search(query, conversation_history)

            if results:
                # Format results for LLM context
                products_info = f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ù…Ù†ØªØ¬ ÙÙŠ Ø§Ù„Ù…Ø®Ø²ÙˆÙ†:\n\n"
                for i, result in enumerate(results, 1):
                    metadata = result['metadata']
                    products_info += f"{i}. {metadata.get('name', 'Ù…Ù†ØªØ¬')}\n"
                    products_info += f"   Ø§Ù„Ø³Ø¹Ø±: {metadata.get('price', 0):.2f} Ø±ÙŠØ§Ù„\n"
                    products_info += f"   Ø§Ù„ÙØ¦Ø©: {metadata.get('category', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}\n"
                    if metadata.get('karat'):
                        products_info += f"   Ø§Ù„Ø¹ÙŠØ§Ø±: {metadata.get('karat')}\n"
                    if metadata.get('weight', 0) > 0:
                        products_info += f"   Ø§Ù„ÙˆØ²Ù†: {metadata.get('weight')} Ø¬Ø±Ø§Ù…\n"
                    if metadata.get('design'):
                        products_info += f"   Ø§Ù„ØªØµÙ…ÙŠÙ…: {metadata.get('design')}\n"
                    if metadata.get('product_url'):
                        products_info += f"   Ø§Ù„Ø±Ø§Ø¨Ø·: {metadata.get('product_url')}\n"
                    products_info += f"   Ø§Ù„ÙˆØµÙ: {metadata.get('description', '')[:150]}...\n\n"

                # Add instruction for LLM
                products_info += "\nØªØ¹Ù„ÙŠÙ…Ø§Øª: ØªØ­Ø¯Ø« Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø¯Ø§ÙØ¦ ÙˆÙ…Ø±Ø­Ø¨ ÙˆÙˆØ¯ÙˆØ¯. Ø§Ø°ÙƒØ± Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒ Ù…Ø¹ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©. ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹. ØªØ°ÙƒØ±: Ø£Ù†Øª ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØªØ±Ø¨Ø· Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ù…Ø§ ØªÙ… Ù…Ù†Ø§Ù‚Ø´ØªÙ‡ Ø³Ø§Ø¨Ù‚Ø§Ù‹."

                return products_info
            else:
                return "Ø£Ø¹ØªØ°Ø±ØŒ Ù„Ù… Ø£Ø¬Ø¯ Ù…Ù†ØªØ¬Ø§Øª ØªØ·Ø§Ø¨Ù‚ Ø·Ù„Ø¨Ùƒ ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹ØªÙ†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠØ©. Ù„ÙƒÙ† Ù„Ø§ ØªÙ‚Ù„Ù‚! ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø´ÙŠØ¡ Ø¢Ø®Ø± Ø£Ùˆ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø¨Ø¯ÙŠÙ„Ø©. Ù…Ø§ Ø±Ø£ÙŠÙƒ Ø£Ù† Ù†Ø¬Ø±Ø¨ Ø¨Ø­Ø«Ø§Ù‹ Ù…Ø®ØªÙ„ÙØ§Ù‹ØŸ ğŸ˜Š"
        else:
            return "Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø­Ø« ØºÙŠØ± Ù…ØªØ§Ø­ Ø­Ø§Ù„ÙŠØ§Ù‹."

    except Exception as e:
        return f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}"

def get_ai_response_with_tools(user_message: str, conversation_history: list) -> str:
    """Get AI response with access to search tools and full conversation context"""
    try:
        # Define the search tool
        search_tool = {
            "type": "function",
            "function": {
                "name": "search_jewelry_products",
                "description": "Search for jewelry products in the store inventory when customer asks about specific products or wants to see what's available",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "Search query for jewelry products"
                        }
                    },
                    "required": ["query"]
                }
            }
        }

        # Build conversation context summary FIRST
        context_summary = ""
        if conversation_history:
            recent_history = conversation_history[-4:] if len(conversation_history) > 4 else conversation_history
            if recent_history:
                context_summary = "ğŸ“‹ Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n"
                for i, msg in enumerate(recent_history):
                    if msg["role"] == "user":
                        context_summary += f"Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù‚Ø§Ù„: {msg['content']}\n"
                    elif msg["role"] == "assistant":
                        context_summary += f"Ø£Ù†Øª Ø£Ø¬Ø¨Øª: {msg['content'][:100]}...\n"

                # Extract key information
                mentioned_products = []
                for msg in recent_history:
                    content = msg['content'].lower()
                    if any(product in content for product in ['Ø®Ø§ØªÙ…', 'Ø¹Ù‚Ø¯', 'Ø³Ù„Ø³Ù„Ø©', 'Ø£Ù‚Ø±Ø§Ø·', 'Ø³ÙˆØ§Ø±']):
                        mentioned_products.append(msg['content'][:150])

                if mentioned_products:
                    context_summary += f"\nğŸ¯ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ø°ÙƒØ±Ù‡Ø§:\n"
                    for product in mentioned_products[-2:]:  # Last 2 product mentions
                        context_summary += f"- {product}\n"

        # Prepare messages with CONTEXT FIRST
        messages = [
            {
                "role": "system",
                "content": f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨ÙŠØ¹Ø§Øª ÙˆØ¯ÙˆØ¯ ÙÙŠ Ù…ØªØ¬Ø± Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª.

{context_summary}

ğŸ”— CRITICAL: Ø¥Ø°Ø§ Ø³Ø£Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¹Ù† "Ø§Ù„Ø³Ø¹Ø±" Ø£Ùˆ "Ø§Ù„Ø£Ù„ÙˆØ§Ù†" Ø£Ùˆ "Ù…ØªÙˆÙØ±" Ø¨Ø¯ÙˆÙ† ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù†ØªØ¬ØŒ
ÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ø¨Ø· Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø¢Ø®Ø± Ù…Ù†ØªØ¬ Ø°ÙƒØ±ØªÙ‡ ÙÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø£Ø¹Ù„Ø§Ù‡.

Ù‚ÙˆØ§Ø¹Ø¯ Ø£Ø³Ø§Ø³ÙŠØ©:
1. Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø£ÙˆÙ„Ø§Ù‹ Ù‚Ø¨Ù„ Ø£ÙŠ Ø´ÙŠØ¡
2. Ø§Ø±Ø¨Ø· Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØºØ§Ù…Ø¶Ø© Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø³Ø§Ø¨Ù‚
3. Ù„Ø¯ÙŠÙƒ Ø£Ø¯Ø§Ø© Ø¨Ø­Ø« Ù„Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙ‚Ø·
4. Ù„Ø§ ØªØ®ØªØ±Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª - Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ù…Ø§ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø£Ùˆ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
5. ÙƒÙ† ÙˆØ¯ÙˆØ¯Ø§Ù‹ ÙˆÙ…ØªØ­Ù…Ø³Ø§Ù‹

ğŸ¯ Ù…Ø«Ø§Ù„: Ø¥Ø°Ø§ Ø°ÙƒØ±Øª Ø®ÙˆØ§ØªÙ… Ø³Ø§Ø¨Ù‚Ø§Ù‹ ÙˆØ³Ø£Ù„ "ÙƒÙ… Ø§Ù„Ø³Ø¹Ø±ØŸ" â†’ Ø£Ø¬Ø¨ Ø¹Ù† Ø£Ø³Ø¹Ø§Ø± Ø§Ù„Ø®ÙˆØ§ØªÙ… Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©."""
            }
        ]

        # Add recent conversation history
        recent_history = conversation_history[-3:] if len(conversation_history) > 3 else conversation_history
        for msg in recent_history:
            if msg["role"] in ["user", "assistant"]:
                messages.append({"role": msg["role"], "content": msg["content"]})

        # Add current user message
        messages.append({"role": "user", "content": user_message})

        # Call OpenAI with function calling
        response = openai.chat.completions.create(
            model="gpt-4",
            messages=messages,
            tools=[search_tool],
            tool_choice="auto",  # Let AI decide when to use tools
            temperature=0.3
        )

        response_message = response.choices[0].message

        # Check if AI wants to use the search tool
        if response_message.tool_calls:
            for tool_call in response_message.tool_calls:
                if tool_call.function.name == "search_jewelry_products":
                    # Extract search query
                    function_args = json.loads(tool_call.function.arguments)
                    search_query = function_args.get("query", "")

                    # Perform search
                    search_result = search_jewelry_products(search_query, conversation_history)

                    # Add tool result to conversation
                    messages.append(response_message)
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": search_result
                    })

                    # Get final response with search results
                    final_response = openai.chat.completions.create(
                        model="gpt-4",
                        messages=messages,
                        temperature=0.3
                    )

                    return final_response.choices[0].message.content

        # No tool call needed, return direct response
        return response_message.content

    except Exception as e:
        return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {e}"

def display_products(products):
    """Display product results in a nice format"""
    if not products:
        return

    cols = st.columns(min(len(products), 3))
    for idx, result in enumerate(products):
        with cols[idx % 3]:
            metadata = result['metadata']

            with st.container():
                st.markdown(f"**{metadata.get('name', 'Ù…Ù†ØªØ¬')}**")
                st.markdown(f"ğŸ’° **{metadata.get('price', 0):.2f} Ø±ÙŠØ§Ù„**")
                st.markdown(f"ğŸ“‚ {metadata.get('category', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")

                # Show additional details
                details = []
                if metadata.get('karat'):
                    details.append(f"Ø§Ù„Ø¹ÙŠØ§Ø±: {metadata.get('karat')}")
                if metadata.get('weight', 0) > 0:
                    details.append(f"Ø§Ù„ÙˆØ²Ù†: {metadata.get('weight')} Ø¬Ø±Ø§Ù…")
                if metadata.get('design'):
                    details.append(f"Ø§Ù„ØªØµÙ…ÙŠÙ…: {metadata.get('design')}")

                if details:
                    st.markdown(f"ğŸ”¹ {' | '.join(details)}")

                # Description (truncated)
                description = metadata.get('description', '')
                if len(description) > 100:
                    description = description[:100] + "..."
                st.markdown(f"ğŸ“ {description}")

                # Match score
                if 'score' in result:
                    st.markdown(f"ğŸ¯ ØªØ·Ø§Ø¨Ù‚: {result['score'] * 100:.1f}%")

                # Add to cart button
                if st.button(f"ğŸ›’ Ø£Ø¶Ù Ù„Ù„Ø³Ù„Ø©", key=f"cart_{result['id']}_{idx}"):
                    st.success("ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù†ØªØ¬ Ù„Ù„Ø³Ù„Ø©! ğŸ›ï¸")

                st.markdown("---")

# Tab selection with radio buttons - using columns to align right
col1, col2 = st.columns([3, 1])

with col2:
    tab_choice = st.radio(
        "Ø§Ø®ØªØ± Ø§Ù„Ù†Ø´Ø§Ø·:",
        ["ğŸ“¸ Ø¨Ø­Ø« Ø¨Ø§Ù„ØµÙˆØ±Ø©", "ğŸ’¬ Ù…Ø­Ø§Ø¯Ø«Ø© Ù†ØµÙŠØ©"],
        horizontal=True,
        index=1  # Default to "Ù…Ø­Ø§Ø¯Ø«Ø© Ù†ØµÙŠØ©" (text search)
    )

st.session_state.active_tab = "chat" if "Ù…Ø­Ø§Ø¯Ø«Ø©" in tab_choice else "image"

if st.session_state.active_tab == "chat":
    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

elif st.session_state.active_tab == "image":
    # Image upload section
    uploaded_image = st.file_uploader(
        "Ø§Ø®ØªØ± ØµÙˆØ±Ø© Ù‚Ø·Ø¹Ø© Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª:",
        type=['png', 'jpg', 'jpeg'],
        help="ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ ØµÙˆØ±Ø© Ù‚Ø·Ø¹Ø© Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª Ù„Ø£Ø­ØµÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù†Ù‡Ø§ Ø£Ùˆ Ø£Ø¬Ø¯ Ù‚Ø·Ø¹ Ù…Ø´Ø§Ø¨Ù‡Ø©"
    )

    if uploaded_image:
        image = Image.open(uploaded_image)
        st.image(image, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø±ÙÙˆØ¹Ø©", width=300)

        if st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ø¨Ø­Ø« Ø¹Ù† Ù…Ø´Ø§Ø¨Ù‡Ø©", type="primary"):
            with st.spinner("ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ù„Ø¨Ø­Ø«..."):
                # Analyze the image
                description = get_image_description(image)

                # Search for similar products using traditional image search
                search_results = search_by_image(pinecone_index, image, top_k=5)

                # Convert results to our format for display
                formatted_results = []
                if search_results:
                    for result in search_results:
                        formatted_results.append({
                            'id': result.id,
                            'score': result.score,
                            'metadata': result.metadata
                        })

                # Create a specific search query based on image details
                # Extract key features for better matching
                search_query = description  # Use the detailed image description directly

                # Use the search tool directly for better matching
                search_results = search_jewelry_products(search_query, st.session_state.messages)

                # Create analysis prompt with search results
                analysis_query = f"""Ù„Ù‚Ø¯ Ø±ÙØ¹ Ø§Ù„Ø¹Ù…ÙŠÙ„ ØµÙˆØ±Ø© Ù„Ù‚Ø·Ø¹Ø© Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª.

ÙˆØµÙ Ø§Ù„ØµÙˆØ±Ø©: {description}

Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù‚Ø·Ø¹ Ù…Ø´Ø§Ø¨Ù‡Ø©:
{search_results}

Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ¹Ø±Ø¶ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø© Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ø£ÙƒØ«Ø± ØªØ´Ø§Ø¨Ù‡Ø§Ù‹ Ù…Ù† Ù†Ø§Ø­ÙŠØ© Ø§Ù„ØªØµÙ…ÙŠÙ… ÙˆØ§Ù„Ù…ÙˆØ§Ø¯ ÙˆØ§Ù„Ø·Ø±Ø§Ø²."""

                # Get response without tool calling (since we already searched)
                response = openai.chat.completions.create(
                    model="gpt-4",
                    messages=[
                        {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨ÙŠØ¹Ø§Øª ÙˆØ¯ÙˆØ¯ ÙÙŠ Ù…ØªØ¬Ø± Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª. Ø­Ù„Ù„ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø© Ø¨Ø­Ù…Ø§Ø³."},
                        {"role": "user", "content": analysis_query}
                    ],
                    temperature=0.3
                )
                bot_response = response.choices[0].message.content

                # Display results
                st.markdown(bot_response)

                # Product cards disabled - all info in conversational text
                # if formatted_results:
                #     display_products(formatted_results)
                # else:
                #     st.info("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù†ØªØ¬Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø©")

                # Add to chat history
                st.session_state.messages.append({
                    "role": "user",
                    "content": "ğŸ–¼ï¸ Ø±ÙØ¹Øª ØµÙˆØ±Ø© Ù‚Ø·Ø¹Ø© Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª"
                })
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": bot_response,
                    "products": formatted_results if formatted_results else None
                })
    else:
        st.info("Ø§Ø®ØªØ± ØµÙˆØ±Ø©")

# Chat input - only show on chat tab
if st.session_state.active_tab == "chat":
    if prompt := st.chat_input("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§..."):
        # Display user message immediately
        with st.chat_message("user"):
            st.markdown(prompt)

        # Display assistant thinking and response
        with st.chat_message("assistant"):
            thinking_placeholder = st.empty()
            thinking_placeholder.markdown("ğŸ¤” Ø£ÙÙƒØ±...")

            # Use function calling approach - ONE LLM call with full context and tools
            response = get_ai_response_with_tools(prompt, st.session_state.messages)

            thinking_placeholder.markdown(response)

        # Add to history after display
        st.session_state.messages.extend([
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": response}
        ])

