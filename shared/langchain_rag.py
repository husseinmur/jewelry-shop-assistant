"""
LangChain-based RAG system for improved jewelry search
Provides better Arabic language understanding and semantic search
"""

import streamlit as st
from typing import List, Dict, Any, Optional
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate
from langchain.chains import RetrievalQA
from pinecone import Pinecone
import openai


class ArabicJewelryRAG:
    """LangChain-based RAG system for Arabic jewelry queries"""

    def __init__(self, pinecone_index, openai_api_key: str):
        """Initialize the RAG system"""
        self.pinecone_index = pinecone_index
        self.openai_api_key = openai_api_key

        # Initialize embeddings - must match existing Pinecone index dimensions (1536)
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002",  # 1536 dimensions - matches existing index
            openai_api_key=openai_api_key
        )

        # Initialize LLM
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0.1,
            openai_api_key=openai_api_key
        )

        # Initialize vector store wrapper
        self.vector_store = None
        self.retriever = None
        self._setup_retriever()

    def _setup_retriever(self):
        """Setup the hybrid retriever (vector + BM25)"""
        try:
            # Get all documents from Pinecone
            docs = self._fetch_all_documents()

            if not docs:
                st.warning("âš ï¸ No documents found in vector store")
                return

            # Create Pinecone vector store wrapper
            self.vector_store = PineconeVectorStore(
                index=self.pinecone_index,
                embedding=self.embeddings,
                text_key="description"
            )

            # Create vector retriever
            vector_retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 8}
            )

            # Create BM25 retriever for keyword matching
            bm25_retriever = BM25Retriever.from_documents(docs)
            bm25_retriever.k = 8

            # Combine both retrievers
            self.retriever = EnsembleRetriever(
                retrievers=[vector_retriever, bm25_retriever],
                weights=[0.7, 0.3]  # Favor semantic over keyword
            )

        except Exception as e:
            st.error(f"âŒ Failed to setup retriever: {e}")

    def _fetch_all_documents(self) -> List[Document]:
        """Fetch all products as LangChain documents"""
        try:
            # Query Pinecone for all products
            results = self.pinecone_index.query(
                vector=[0.0] * 1536,  # Dummy vector
                top_k=1000,  # Get many results
                include_metadata=True
            )

            documents = []
            for match in results.matches:
                metadata = match.metadata

                # Create rich text content for better search
                content = self._create_document_content(metadata)

                doc = Document(
                    page_content=content,
                    metadata={
                        "id": match.id,
                        "name": metadata.get("name", ""),
                        "category": metadata.get("category", ""),
                        "price": metadata.get("price", 0),
                        "karat": metadata.get("karat", ""),
                        "weight": metadata.get("weight", 0),
                        "design": metadata.get("design", ""),
                        "style": metadata.get("style", ""),
                        "product_url": metadata.get("product_url", ""),
                        "score": getattr(match, 'score', 0)
                    }
                )
                documents.append(doc)

            return documents

        except Exception as e:
            st.error(f"Error fetching documents: {e}")
            return []

    def _create_document_content(self, metadata: Dict) -> str:
        """Create rich searchable content from product metadata"""
        content_parts = []

        # Product name
        if metadata.get("name"):
            content_parts.append(f"Ø§Ù„Ù…Ù†ØªØ¬: {metadata['name']}")

        # Category
        if metadata.get("category"):
            content_parts.append(f"Ø§Ù„Ù†ÙˆØ¹: {metadata['category']}")

        # Materials and specifications
        if metadata.get("karat"):
            content_parts.append(f"Ø§Ù„Ø¹ÙŠØ§Ø±: {metadata['karat']}")

        if metadata.get("weight", 0) > 0:
            content_parts.append(f"Ø§Ù„ÙˆØ²Ù†: {metadata['weight']} Ø¬Ø±Ø§Ù…")

        if metadata.get("design"):
            content_parts.append(f"Ø§Ù„ØªØµÙ…ÙŠÙ…: {metadata['design']}")

        if metadata.get("style"):
            content_parts.append(f"Ø§Ù„Ø·Ø±Ø§Ø²: {metadata['style']}")

        # Main description
        if metadata.get("description"):
            content_parts.append(f"Ø§Ù„ÙˆØµÙ: {metadata['description']}")

        # Price
        if metadata.get("price"):
            content_parts.append(f"Ø§Ù„Ø³Ø¹Ø±: {metadata['price']} Ø±ÙŠØ§Ù„")

        return " | ".join(content_parts)

    def search(self, query: str, max_results: int = 5) -> List[Dict]:
        """Search for products using LangChain RAG"""
        try:
            if not self.retriever:
                return []

            # Enhanced query processing
            processed_query = self._enhance_query(query)

            # Retrieve relevant documents
            docs = self.retriever.get_relevant_documents(processed_query)

            # Convert back to our format
            results = []
            for doc in docs[:max_results]:
                # Create result object similar to Pinecone format
                result = {
                    'id': doc.metadata.get('id', ''),
                    'score': doc.metadata.get('score', 0.5),  # Default score
                    'metadata': {
                        'name': doc.metadata.get('name', ''),
                        'category': doc.metadata.get('category', ''),
                        'price': doc.metadata.get('price', 0),
                        'karat': doc.metadata.get('karat', ''),
                        'weight': doc.metadata.get('weight', 0),
                        'design': doc.metadata.get('design', ''),
                        'style': doc.metadata.get('style', ''),
                        'product_url': doc.metadata.get('product_url', ''),
                        'description': doc.page_content
                    }
                }
                results.append(result)

            return results

        except Exception as e:
            st.error(f"Search error: {e}")
            return []

    def _enhance_query(self, query: str) -> str:
        """Enhance query for better Arabic search"""
        # Expand common Arabic jewelry terms
        expansions = {
            "Ø³Ù„Ø§Ø³Ù„": "Ø³Ù„Ø§Ø³Ù„ Ø¹Ù‚ÙˆØ¯ Ù‚Ù„Ø§Ø¦Ø¯ Ø³Ù„Ø³Ù„Ø© Ù‚Ù„Ø§Ø¯Ø© Ø¹Ù‚Ø¯",
            "Ø®ÙˆØ§ØªÙ…": "Ø®ÙˆØ§ØªÙ… Ø®Ø§ØªÙ… Ø¯Ø¨Ù„",
            "Ø§Ø³Ø§ÙˆØ±": "Ø§Ø³Ø§ÙˆØ± Ø³ÙˆØ§Ø± Ø§Ø³ÙˆØ±Ø©",
            "Ø§Ù‚Ø±Ø§Ø·": "Ø§Ù‚Ø±Ø§Ø· Ù‚Ø±Ø· Ø­Ù„Ù‚",
            "Ø°Ù‡Ø¨": "Ø°Ù‡Ø¨ Ø°Ù‡Ø¨ÙŠ Ø°Ù‡Ø¨ÙŠØ©",
            "ÙØ¶Ø©": "ÙØ¶Ø© ÙØ¶ÙŠ ÙØ¶ÙŠØ©",
            "Ø¨Ø³ÙŠØ·": "Ø¨Ø³ÙŠØ· Ø¨Ø³Ø§Ø·Ø© Ù†Ø§Ø¹Ù…",
            "ÙØ§Ø®Ø±": "ÙØ§Ø®Ø± ÙØ®Ù… Ø±Ø§Ù‚ÙŠ Ø£Ù†ÙŠÙ‚"
        }

        enhanced_query = query
        for term, expansion in expansions.items():
            if term in query:
                enhanced_query = f"{query} {expansion}"
                break

        return enhanced_query

    def conversational_search(self, query: str, conversation_history: List = None) -> tuple:
        """
        Perform conversational search with context awareness
        Returns: (answer, search_results)
        """
        try:
            # Get search results
            search_results = self.search(query, max_results=5)

            if not search_results:
                return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù†ØªØ¬Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ø·Ù„Ø¨Ùƒ ÙÙŠ Ù…Ø®Ø²ÙˆÙ†Ù†Ø§ Ø§Ù„Ø­Ø§Ù„ÙŠ. Ø¬Ø±Ø¨ Ù…ØµØ·Ù„Ø­Ø§Øª Ø£Ø®Ø±Ù‰ Ø£Ùˆ ØªØµÙØ­ Ù…Ø¬Ù…ÙˆØ¹ØªÙ†Ø§.", []

            # Create context from search results
            context = self._create_context_from_results(search_results)

            # Build conversation context
            conversation_context = ""
            if conversation_history:
                recent_messages = conversation_history[-4:] if len(conversation_history) > 4 else conversation_history
                for msg in recent_messages:
                    if msg.get("role") in ["user", "assistant"]:
                        content = msg.get("content", "")[:150]  # Limit length
                        conversation_context += f"{msg['role']}: {content}\n"

            # Create conversational prompt
            prompt = ChatPromptTemplate.from_template("""
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø¨ÙŠØ¹Ø§Øª ÙˆØ¯ÙˆØ¯ ÙˆÙ…ØªØ­Ù…Ø³ ÙÙŠ Ù…ØªØ¬Ø± Ù…Ø¬ÙˆÙ‡Ø±Ø§Øª! ðŸ’Ž
ØªØ­Ø¨ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙÙŠ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø¬Ù…Ù„ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„ØªÙŠ ØªÙ†Ø§Ø³Ø¨Ù‡Ù….

{history_section}

Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª:
{context}

Ø´Ø®ØµÙŠØªÙƒ ÙˆØ£Ø³Ù„ÙˆØ¨Ùƒ:
- Ù…Ø±Ø­Ø¨ ÙˆÙˆØ¯ÙˆØ¯ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙˆÙ‚Ø§Øª
- Ù…ØªØ­Ù…Ø³ Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø¬Ù…ÙŠÙ„Ø©
- ØªØ³ØªØ®Ø¯Ù… Ø¹Ø¨Ø§Ø±Ø§Øª Ø¯Ø§ÙØ¦Ø© ÙˆÙ…Ø´Ø¬Ø¹Ø©
- ØªÙ‡ØªÙ… Ø­Ù‚Ø§Ù‹ Ø¨Ø¥Ø³Ø¹Ø§Ø¯ Ø§Ù„Ø¹Ù…ÙŠÙ„
- ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ø³ØªÙ…Ø±Ø§Ø±ÙŠØ© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©

Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø©:
1. Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø¹Ù„Ø§Ù‡
2. Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø£Ùˆ ØªØ¶ÙŠÙ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©
3. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø© Ù…Ø­Ø¯Ø¯Ø©ØŒ Ù‚Ù„ "Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©" Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ¯ÙˆØ¯Ø©
4. Ø§Ø°ÙƒØ± Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙˆØ§Ù„Ø£Ø³Ø¹Ø§Ø± Ø¹Ù†Ø¯ Ø§Ù„ØªÙˆØµÙŠØ© Ù…Ø¹ Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ø­Ù…Ø§Ø³
5. ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø· Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªÙˆÙØ±Ø§Ù‹ Ù„Ù„Ù…Ù†ØªØ¬
6. Ø§Ø¨Ø¯Ø£ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ Ø¨ØªØ±Ø­ÙŠØ¨ Ø¯Ø§ÙØ¦ ÙˆØ§Ø®ØªØªÙ…Ù‡Ø§ Ø¨Ø¹Ø±Ø¶ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¥Ø¶Ø§ÙÙŠØ©
7. Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø«Ù„ "ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ø£Ù† Ø£Ø³Ø§Ø¹Ø¯Ùƒ" Ùˆ "Ø£ØªÙ…Ù†Ù‰ Ø£Ù† ØªÙ†Ø§Ù„ Ø¥Ø¹Ø¬Ø§Ø¨Ùƒ"
8. Ø§Ø±Ø¨Ø· Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø¨Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©

Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¹Ù…ÙŠÙ„: {question}

Ø¥Ø¬Ø§Ø¨ØªÙƒ Ø§Ù„ÙˆØ¯ÙˆØ¯Ø©:
""")

            # Generate response
            chain = prompt | self.llm | StrOutputParser()
            # Prepare history section
            history_section = f"Ù…Ø­Ø§Ø¯Ø«Ø© Ø³Ø§Ø¨Ù‚Ø©:\n{conversation_context}" if conversation_context else "Ø¨Ø¯Ø§ÙŠØ© Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø©"

            response = chain.invoke({
                "context": context,
                "question": query,
                "history_section": history_section
            })

            return response, search_results

        except Exception as e:
            return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ: {e}", []

    def _create_context_from_results(self, results: List[Dict]) -> str:
        """Create formatted context from search results"""
        context_parts = []

        for i, result in enumerate(results, 1):
            metadata = result['metadata']
            url_info = f"- Ø§Ù„Ø±Ø§Ø¨Ø·: {metadata.get('product_url')}" if metadata.get('product_url') else ""

            context_parts.append(f"""
Ø§Ù„Ù…Ù†ØªØ¬ {i}:
- Ø§Ù„Ø§Ø³Ù…: {metadata.get('name', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
- Ø§Ù„Ø³Ø¹Ø±: {metadata.get('price', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')} Ø±ÙŠØ§Ù„
- Ø§Ù„Ù†ÙˆØ¹: {metadata.get('category', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
- Ø§Ù„Ø¹ÙŠØ§Ø±: {metadata.get('karat', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
- Ø§Ù„ÙˆØ²Ù†: {metadata.get('weight', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')} Ø¬Ø±Ø§Ù…
- Ø§Ù„ØªØµÙ…ÙŠÙ…: {metadata.get('design', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}
- Ø§Ù„Ø·Ø±Ø§Ø²: {metadata.get('style', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}{url_info}
- Ø§Ù„ÙˆØµÙ: {metadata.get('description', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')[:200]}...
""")

        return "\n".join(context_parts)


def init_langchain_rag(pinecone_index, openai_api_key: str) -> ArabicJewelryRAG:
    """Initialize and return LangChain RAG system"""
    try:
        rag_system = ArabicJewelryRAG(pinecone_index, openai_api_key)
        return rag_system
    except Exception as e:
        st.error(f"Failed to initialize LangChain RAG: {e}")
        return None